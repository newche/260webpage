---
title: "Figures"
output: html_document
---


```{r include=FALSE}
# data cleaning
library(tidyverse)
library(table1)
library(gridExtra)
library(ggplot2)
library(ggpubr)
library(glmnet)
library(purrr)
library(caret)
library(randomForest)
library(knitr)
library(kableExtra)

heart <- read.csv("heart.csv", header=T)
# omit NA
heart <- heart|> na.omit()
# response variable y
y <- heart$HeartDisease 

heart <- heart |> mutate(FastingBS = factor(FastingBS, levels=c(0,1), labels=c("Blood Sugar > 120 mg/dl", "otherwise")),
                         HeartDisease = factor(HeartDisease, levels=c(0,1))) |> filter(Cholesterol>0, RestingBP>0, Oldpeak>=0)

```

```{r include=FALSE}
# Logistic model with all predictors
M1 = glm(HeartDisease ~ . , data = heart, family = binomial)
summary(M1)

set.seed(100)
#randomly selected 80% as training set for hyper-parameters
trainset <- sample_frac(heart, 0.8)
# Logistic model with LASSO predictors
lambda_grid = 10^seq(5,-2,length=100)
x = model.matrix(HeartDisease~., data=trainset)
y = as.integer(trainset$HeartDisease)
lasso.mod=glmnet(x, y, alpha=1, family="binomial", lambda=lambda_grid)
cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
lasso.mod2=glmnet(x,y, family="binomial", alpha=1)
cv_fit <- cv.glmnet(x,y,alpha=1, lambda=lambda_grid, family="binomial")
opt_lambda <- cv.out$lambda[which.min(cv.out$cvm)]
opt_lambda #0.007847
lasso.coef = predict(lasso.mod2, type = "coefficients", s = opt_lambda)
lasso.coef

M2 = glm(HeartDisease ~ .-RestingECG - MaxHR , data = heart, family = binomial)
summary(M2)

```

```{r include=FALSE}
set.seed(10)
ks <- seq(1,11,1)
trControl <- trainControl(method="cv", number=10)
fit_knn <- train(as.factor(HeartDisease) ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:11),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = trainset)
best_K <- fit_knn$results$k[which.max(fit_knn$results$Accuracy)]
best_K #8
M3 <- knn3(as.factor(HeartDisease) ~ ., data=heart, k=8)
```

```{r include=FALSE}
# conduct 10 fold CV to compare test performance
set.seed(10)
K = 10
n = nrow(heart)
n_models = 4
permutation = sample(1:n)  
test_error_fold = matrix(0, K, n_models)
sensititivy_fold = matrix(0, K, n_models)
specificity_fold = matrix(0, K, n_models)
for (j in 1:K) {
  # 1. extract indices of units in the pseudo-test set for split j
  pseudotest = heart[permutation[floor((j-1)*n/K+1) : floor(j*n/K)], ]  
  # 2. extract indices of units in the pseudo-training set for split j
  pseudotrain = heart[setdiff(1:n, pseudotest), ]
  # 3. fit all models of interest
  m1 = glm(as.factor(HeartDisease) ~ ., data = pseudotrain, family = binomial)
  m2 = glm(as.factor(HeartDisease) ~ .-RestingECG - MaxHR , data = pseudotrain, family = binomial)
  m3 = knn3(as.factor(HeartDisease) ~ ., data=pseudotrain, k=8)
  m4 = randomForest(as.factor(HeartDisease)~., mtry = 2, data = pseudotrain, importance = T)
  # 4. Compute Performance on each model
  for (i in 1:n_models) {
    if (i==3) {
      probabilities = eval(m3) |> predict(pseudotest, type="class")
      results = confusionMatrix(probabilities, true.classes, positive = "1")
      test_error_fold[j, i] = 1 - results$overall["Accuracy"]
      sensititivy_fold[j, i] = results$byClass["Sensitivity"]
      specificity_fold[j, i] = results$byClass["Specificity"]
    }
    if (i==4) {
      probabilities = eval(m4) |> predict(pseudotest, type = "response")
      results = confusionMatrix(probabilities, true.classes, positive = "1")
      test_error_fold[j, i] = 1 - results$overall["Accuracy"]
      sensititivy_fold[j, i] = results$byClass["Sensitivity"]
      specificity_fold[j, i] = results$byClass["Specificity"]
    }
    if(i==1 | i==2 ) {
      probabilities = eval(parse(text = paste0("m", i))) |> predict(pseudotest, type = "response")
      predicted.classes = factor(ifelse(probabilities > 0.5, 1, 0), levels = 0:1)
      true.classes = factor(pseudotest$HeartDisease, levels = 0:1)
      results = confusionMatrix(predicted.classes, true.classes, positive = "1")
      test_error_fold[j, i] = 1 - results$overall["Accuracy"]
      sensititivy_fold[j, i] = results$byClass["Sensitivity"]
      specificity_fold[j, i] = results$byClass["Specificity"]
    }
    
    #results = confusionMatrix(predicted.classes, true.classes, positive = "1")
    #test_error_fold[j, i] = 1 - results$overall["Accuracy"]
    #sensititivy_fold[j, i] = results$byClass["Sensitivity"]
    #specificity_fold[j, i] = results$byClass["Specificity"]
  }
}
# average across splits to obtain CV estimate of test MSE
test_error_cvs = round(colMeans(test_error_fold), 5)
sensitivity_cvs = round(colMeans(sensititivy_fold), 5)
specificity_cvs = round(colMeans(specificity_fold), 5)  
models = c("Model 1", "Model 2", "Model 3", "Model 4")
descriptions = c("Logistic - All", "Logistic - Lasso", "KNN", "Random Forest")
results = cbind(models, descriptions, test_error_cvs, sensitivity_cvs, specificity_cvs)
colnames(results) = c("Model", "Description", "Test Error Rate", "Sensitivity", "Specificity")
#results
```



### List of Figures


**Figure 1. Exploratory Data Analysis - Cholesterol, Age, Sex, Chest Pain**


```{r echo=FALSE}
p_cho <- heart |> ggplot(aes(x=Cholesterol,group=HeartDisease, fill=HeartDisease)) + geom_density(adjust=1,alpha=.4) + theme_classic() + labs(title="Cholesterol Density by Heart Disease") + xlab("Cholesterol(mm/dl)") 

p_age <- heart |> ggplot(aes(x=Age,group=HeartDisease, fill=HeartDisease)) + geom_density(adjust=1,alpha=.4) + theme_classic() + labs(title="Age Density by heart disease") + xlab("age(year)")
p_sex <- heart |> ggplot(aes(x=HeartDisease, fill=Sex)) + geom_bar(alpha=.6) + labs(title="Bar of Heart Disease by Sex") + scale_fill_manual(values=c("firebrick2", "dodgerblue1")) + theme_classic()  + coord_flip()
p_chest <- heart |> ggplot(aes(x=HeartDisease, fill=ChestPainType)) + geom_bar(alpha=.7) + labs(title="Bar of Heart Disease by Chest Pain") + scale_fill_manual(values=c( "#4E84C4", "#D55E00","#C3D7A4", "#FFDB6D")) + theme_classic()  + coord_flip()
#p_exer <- heart |> ggplot(aes(x=HeartDisease, fill=ExerciseAngina)) + geom_bar(alpha=.6) + labs(title="bar plot #of heart disease by Exercise Angina groups") + scale_fill_manual(values=c("#56B4E9", "#F0E442")) + #theme_classic() + theme(aspect.ratio = 1/5) + coord_flip()
ggarrange(p_cho, p_age, p_sex, p_chest, ncol = 2, nrow=2)
```




**Figure 2. Each Variable's Influence on Random Forest Model**


```{r echo=FALSE}
set.seed(2)
grid <- data.frame(mtry = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
control <- trainControl(method="cv", number = 10)
train_rf <-  train(x, as.factor(y), 
                   method = "rf", 
                   ntree = 150,
                   trControl = control,
                   tuneGrid = grid,
                   nSamp = 5000)
bestmtry <- train_rf$results$mtry[which.max(train_rf$results$Accuracy)]
#bestmtry #2
M4 <- randomForest(HeartDisease~., mtry = 2, data = heart, importance = T)
varImpPlot(M4, main = "Each variable's influence on Random Forest model")

```




**Figure 3. Error Rate vs. Number of Trees in Random Forest Model**


```{r echo=FALSE}
plot(M4, main="Error rate vs. number of trees in Random Forest model")
```



**Figure 4. ROC Curve Comparison on a Single Test Set**


```{r include=FALSE}
library(pROC)

set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(heart), replace=TRUE, prob=c(0.7,0.3))
train  <- heart[sample, ]
test   <- heart[!sample, ]

m1 = glm(as.factor(HeartDisease) ~ ., data = test, family = binomial)
m2 = glm(as.factor(HeartDisease) ~ .-RestingECG - MaxHR , data = test, family = binomial)
m3 = knn3(as.factor(HeartDisease) ~ ., data=test, k=8)
m4 = randomForest(as.factor(HeartDisease)~., mtry = 2, data = test, importance = T)


probabilities1 = eval(m1) |> predict(test, type = "response")
predicted.classes1 = factor(ifelse(probabilities1 > 0.5, 1, 0), levels = 0:1)

probabilities2 = eval(m2) |> predict(test, type = "response")
predicted.classes2 = factor(ifelse(probabilities2 > 0.5, 1, 0), levels = 0:1)

probabilities3 = eval(m3) |> predict(test, type="class")
probabilities4 = eval(m4) |> predict(test, type = "response")
true_values <- test$HeartDisease

roc_rf = roc(as.numeric(true_values), as.numeric(probabilities4)) #  ROC curve creation
roc_kNN = roc(as.numeric(true_values), as.numeric(probabilities3)) 
roc_Logistic_Lasso = roc(as.numeric(true_values), as.numeric(predicted.classes2))
roc_Logistic_All = roc(as.numeric(true_values), as.numeric(predicted.classes1))
```

```{r echo=FALSE}

ggroc(list("Random Forest" = roc_rf, "kNN" = roc_kNN, "Logistic All" = roc_Logistic_All, "Logistic Lasso" = roc_Logistic_Lasso)) +
  theme(legend.title = element_blank()) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color = "black", linetype = "dashed") +
  xlab("Sensitivity") +
  ylab("Specificity") +
  ggtitle("ROC curve comparison") + theme_classic()
  

```




**Figure 5. Confusion Matrix of Best Model - Random Forest**


```{r echo=FALSE}
p44 = eval(M4) |> predict(test, type = "response")
x <- confusionMatrix(p44, true_values, positive = "1")
table <- data.frame(x$table)

plotTable <- table %>%
  mutate(goodbad = ifelse(table$Prediction == table$Reference, "good", "bad")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

ggplot(data = plotTable, mapping = aes(x = Reference, y = Prediction, fill = goodbad, alpha = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_manual(values = c(good = "blue3", bad = "pink")) +
  theme_bw() +
  xlim(rev(levels(table$Reference)))

```